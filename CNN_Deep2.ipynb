{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/disk/scratch/mlp/miniconda2/bin/python\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Utilities\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check necessary environment variables are defined\n",
    "assert 'MLP_DATA_DIR' in os.environ, 'An environment variable MLP_DATA_DIR must be set to the path containing MLP data before running script.'\n",
    "# assert 'OUTPUT_DIR' in os.environ, 'An environment variable OUTPUT_DIR must be set to the path to write output to before running script.'\n",
    "save_point = '/home/s1687487/0307/3/exp_2/' #'/home/s1687487/0228/0/exp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "# batch_size = 500\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "batch_size = 128\n",
    "from mlp.data_providers import MSD10GenreDataProvider, MSD25GenreDataProvider\n",
    "train_data = MSD10GenreDataProvider('train', batch_size = batch_size, max_num_batches=2)\n",
    "valid_data = MSD10GenreDataProvider('valid', batch_size = batch_size, max_num_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "# training_iters = 100000\n",
    "training_epochs = 2\n",
    "display_step = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 25 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 120 # timesteps\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def experiment():    \n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    tf.add_to_collection('vars', weights['out'])\n",
    "    tf.add_to_collection('vars', biases['out'])\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    def ConvNetwork(x, weights, biases):\n",
    "        # Convolutional Layer #1\n",
    "#         print 'x shape =', x.get_shape()\n",
    "        input_layer = tf.reshape(x, [-1, n_steps, n_input, 1])\n",
    "#         print 'input_layer shape =', input_layer.get_shape()\n",
    "        conv1 = tf.contrib.layers.conv2d(input_layer,filters1_num,kernel1_size,padding='SAME')#,activation=tf.nn.elu)        \n",
    "#         print 'conv1 shape =', conv1.get_shape()\n",
    "        # Pooling Layer #1\n",
    "        pool1 = tf.contrib.layers.max_pool2d(conv1, pool1_size, [2,1])\n",
    "#         print 'pool1 shape =', pool1.get_shape()\n",
    "        \n",
    "        \n",
    "        # Conv #2\n",
    "#x shape = (?, 120, 25)\n",
    "# input_layer shape = (?, 120, 25, 1)\n",
    "# conv1 shape = (?, 120, 25, 32)\n",
    "# pool1 shape = (?, 60, 25, 32)  \n",
    "\n",
    "        conv2 = tf.contrib.layers.conv2d(pool1,filters2_num,kernel2_size,padding='VALID')#,activation=tf.nn.elu)        \n",
    "#         print 'conv2 shape =', conv2.get_shape()\n",
    "        pool2 = tf.contrib.layers.max_pool2d(conv2, pool2_size, [3,1])\n",
    "#         print 'pool2 shape =', pool2.get_shape()\n",
    "                        \n",
    "        # Dense Layer\n",
    "        reduc_t, reduc_inp, n_ch = pool2.get_shape()[1:4].as_list()\n",
    "        multiple = reduc_t * reduc_inp * n_ch\n",
    "        pool_flat = tf.reshape(pool2, [-1, multiple])\n",
    "#         print 'pool1_flat shape =', pool1_flat.get_shape()\n",
    "        \n",
    "        w_dense = tf.Variable(\n",
    "            tf.truncated_normal([multiple, n_units1], stddev=2. / (multiple + n_units1)**0.5), \n",
    "            'w_dense')\n",
    "        b_dense = tf.Variable(tf.zeros([n_units1]), 'b_dense')\n",
    "    \n",
    "        dense = tf.nn.relu(tf.matmul(pool_flat, w_dense) + b_dense)\n",
    "#         print 'dense shape =', dense.get_shape()\n",
    "        dropout = tf.contrib.layers.dropout(dense, keep_prob=1.- do1_p)\n",
    "#         print 'dropout shape = ', dropout.get_shape()\n",
    "        \n",
    "        final1 = tf.matmul(dropout, weights)\n",
    "#         print 'final1 shape = ', final1.get_shape()\n",
    "        final = final1 + biases\n",
    "#         print 'final shape = ', final.get_shape()\n",
    "        \n",
    "        return final\n",
    "\n",
    "    pred = ConvNetwork(x, weights['out'], biases['out'])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Performance pickles\n",
    "    err_tr = np.zeros(training_epochs)\n",
    "    acc_tr = np.zeros(training_epochs)\n",
    "    err_val = np.zeros(training_epochs)\n",
    "    acc_val = np.zeros(training_epochs)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Optimization Starts! CNN_(depth,n_filter1,n_filter2,p_DO,n_hidden)=(2,{0},{1},{2},{3})\".format(filters1_num,filters2_num,do1_p,n_hidden))\n",
    "        sess.run(init) #initial run\n",
    "\n",
    "        for e in range(training_epochs):                              \n",
    "            if e == 0:\n",
    "                start = time.time()\n",
    "                num_batches_tr = 0\n",
    "                num_batches_val = 0\n",
    "\n",
    "            running_error = 0.\n",
    "            running_accuracy = 0.\n",
    "            for input_batch, target_batch in train_data:\n",
    "                if e == 0:\n",
    "                    num_batches_tr += 1\n",
    "                input_batch = input_batch.reshape((batch_size,n_steps,n_input))\n",
    "\n",
    "                _, batch_error, batch_acc = sess.run(\n",
    "                    [optimizer, cost, accuracy], \n",
    "                    feed_dict={x: input_batch, y: target_batch})\n",
    "                running_error += batch_error\n",
    "                running_accuracy += batch_acc\n",
    "  \n",
    "            running_error /= num_batches_tr\n",
    "            err_tr[e] = running_error\n",
    "            running_accuracy /= num_batches_tr\n",
    "            acc_tr[e] = running_accuracy\n",
    "\n",
    "            if ((e+1) % display_step == 0) or (e == 0):\n",
    "                end = time.time()\n",
    "                print('End of epoch {0:02d}: err(train)={1:.2f} acc(train)={2:.2f} ,,, taking {3:.1f}(sec)'\n",
    "                      .format(e + 1, running_error, running_accuracy, end-start))\n",
    "\n",
    "                start = time.time()            \n",
    "\n",
    "                valid_error = 0.\n",
    "                valid_accuracy = 0.\n",
    "                for input_batch, target_batch in valid_data:\n",
    "                    if e == 0:\n",
    "                        num_batches_val += 1\n",
    "                    input_batch = input_batch.reshape((batch_size,n_steps,n_input))\n",
    "\n",
    "                    batch_error, batch_acc = sess.run(\n",
    "                        [cost, accuracy], \n",
    "                        feed_dict={x: input_batch, y: target_batch})\n",
    "                    valid_error += batch_error\n",
    "                    valid_accuracy += batch_acc\n",
    "                valid_error /= num_batches_val\n",
    "                err_val[e] = valid_error\n",
    "                valid_accuracy /= num_batches_val\n",
    "                acc_val[e] = valid_accuracy\n",
    "\n",
    "                end = time.time()\n",
    "                print('                 err(valid)={0:.2f} acc(valid)={1:.2f} ,,, taking {2:.1f}(sec) '\n",
    "                       .format(valid_error, valid_accuracy, end-start))\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "        #Save model - save session        \n",
    "        sname = \"params_cnnDeep2_n_filter12,p_DO,n_hidden,,,\"+ str([filters1_num,filters2_num,do1_p,n_hidden])\n",
    "        saver.save(sess, sname)\n",
    "#             saver.save(sess, save_point+sname)\n",
    "        #...performance as well - pickle\n",
    "        prf_dict = {\"err_tr\": err_tr, \"err_val\": err_val, \"acc_tr\": acc_tr, \"acc_val\": acc_val}\n",
    "        fname = \"cnnDeep2_performances_n_filter12,p_DO,n_hidden,,,\"+ str([filters1_num,filters2_num,do1_p,n_hidden])\n",
    "        pickle.dump(prf_dict, open(fname+\".p\",\"wb\"))\n",
    "#             pickle.dump(prf_dict, open(save_point+fname,\"wb\"))\n",
    "\n",
    "        #Reset the data\n",
    "        train_data.reset()\n",
    "        valid_data.reset()\n",
    "        print ''\n",
    "\n",
    "    #Reset the graph per each iteration\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Starts! CNN_(depth,n_filter1,n_filter2,p_DO,n_hidden)=(2,32,128,0.0,30)\n",
      "End of epoch 01: err(train)=4.88 acc(train)=0.09 ,,, taking 1.3(sec)\n",
      "                 err(valid)=3.79 acc(valid)=0.13 ,,, taking 0.2(sec) \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'list' objects",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5fdf14a16694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mkernel1_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0mkernel2_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim_t2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                         \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimization Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9cbcb2ca4581>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m#Save model - save session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilters1_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilters2_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo1_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0msname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"params_cnnDeep2_n_filter12,p_DO,n_hidden,,,\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m#             saver.save(sess, save_point+sname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate 'str' and 'list' objects"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "filters1_num_list = [32]#[6,12,24,36]\n",
    "filters2_num_list = [128,32,16]\n",
    "\n",
    "dim_t_list = [3]#[3,5,7,13]\n",
    "dim_t2_list = [4]\n",
    "\n",
    "do1_p_list = [0.,0.5]\n",
    "\n",
    "pool1_size = [2,1]\n",
    "pool2_size = [2,1]\n",
    "\n",
    "n_hidden_list = [30,64,128,256]\n",
    "\n",
    "for n_hidden in n_hidden_list:\n",
    "    n_units1 = n_hidden\n",
    "    for dim_t in dim_t_list:\n",
    "        for dim_t2 in dim_t2_list:\n",
    "            for filters1_num in filters1_num_list:\n",
    "                for filters2_num in filters2_num_list:\n",
    "                    for do1_p in do1_p_list:\n",
    "                        kernel1_size = [dim_t,n_input]\n",
    "                        kernel2_size = [dim_t2,n_input]\n",
    "                        experiment()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlp]",
   "language": "python",
   "name": "conda-env-mlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
